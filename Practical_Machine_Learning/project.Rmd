---
title: "Practical Machine Learning Project"
author: "HHH"
date: "October 24, 2015"
output: html_document
---


The goal of this project is to build the model to predict the exercise type. The datasets are given by sensor measurements installed on the belt, forearm, arm, and dumbell of 6 participants, and can be classified to 5 different categories of exercise, labeled as "classe=A, B, C, D and E". In the following, I will use the "Random Forest" algorithm to train the data, constuct the model and further use it to predict the outcome for the test sets. During the traning, I implemented "K-fold cross-validation" to examine the accuracy of our model. I will show that given the simulation, the fitted model is reliable to predict a new test set.



First, one can load the training datasets and testing datasets, and store as "data_training" and "data_testing"
```{r}
data_training <- read.csv("pml-training.csv")
data_testing  <- read.csv("pml-testing.csv")
dim(data_training)
dim(data_testing)
```
It seems there are 160 features in the training and test sets. But by typeing "names(data_training)" and "names(data_testing)", we can infer there are useful 159 features in the datasets. The last variable in the training set is "classe" as the $y$ outcome. The last variable in the testing set is "id" which lables problem index.


Next one can observe what features in the training sets may be important variable to determine the exercise type. Comparing the test and training sets, I found there are 52 possible features which may play roles to relate the exercise categories. The corresponding variable names are "gyros_A_$\alpha$", "accel_A_$\alpha$", "magnet_A_$\alpha$", and, "roll_A", "pitch_A", "yaw_A" and "total_accel_A", where "A" are "forearm", "dumbbell", "arm", "belt", and  "$\alpha=x,y,z$". Load the "caret"" and "randomForest"" packages in R
```{r}
library(ggplot2)
library(lattice)
library(caret)
library(randomForest)
forearm_var <- c("gyros_forearm_x","gyros_forearm_y","gyros_forearm_z","accel_forearm_x","accel_forearm_y","accel_forearm_z","magnet_forearm_x","magnet_forearm_y","magnet_forearm_z","roll_forearm","pitch_forearm","yaw_forearm","total_accel_forearm")
dumbbell_var <- c("gyros_dumbbell_x","gyros_dumbbell_y","gyros_dumbbell_z","accel_dumbbell_x","accel_dumbbell_y","accel_dumbbell_z","magnet_dumbbell_x","magnet_dumbbell_y","magnet_dumbbell_z","roll_dumbbell","pitch_dumbbell","yaw_dumbbell","total_accel_dumbbell")
arm_var <- c("gyros_arm_x","gyros_arm_y","gyros_arm_z","accel_arm_x","accel_arm_y","accel_arm_z","magnet_arm_x","magnet_arm_y","magnet_arm_z","roll_arm","pitch_arm","yaw_arm","total_accel_arm")
belt_var <- c("gyros_belt_x","gyros_belt_y","gyros_belt_z","accel_belt_x","accel_belt_y","accel_belt_z","magnet_belt_x","magnet_belt_y","magnet_belt_z","roll_belt","pitch_belt","yaw_belt","total_accel_belt")
variable <- c(belt_var,arm_var,dumbbell_var,forearm_var)
datavar        <- data_training[variable]
datavar$classe <- data_training$classe
```
Although the training set has 159 features, other variables show either "NA" or nothing; therefore one can ignore them in building models. We can withdraw the 52 features, and add the "classe" (as y) to generate a data frame called "datavar", rather than directly training "data_training" which has 159 features. Now the reorganized dataset "datavar" has 53 columns (52 features + $y$):
```{r}
dim(datavar)[2]
n_sample <- dim(datavar)[1]
n_sample
```

The number of examples (samples) are 19,622. But we can split the data into k pieces to implement "K-fold cross-validation". In this algorithm, one of the pieces is regarded as cross-validation sets, and the remianing K-1 ones is used for training sets, in order to examine the sample error.
Below I set "k_fold=5" and evenly parition the "datavar" into 5 pieces by randomly assigning a index array:
```{r}
k_fold =5
datavar$id <- sample(1:k_fold,n_sample,replace=TRUE)
table(datavar$id)
```
Then the "datavar" has an additional (artifical) column named "id". So each sample has a lable "id=1,2...5", assigned randomly. For example, we can see the "id" in the first 20 rows of the "datavar" set:
```{r}
head(datavar$id,20)
```
Then we can easily split the "datavar" into 5 pieces by "id" category.

Next step we perform a loop to run "K-fold cross-validation". In each iteration one piece (k-th piece) is set as cross-validation, named as "crossval" and the others are combined as the training set named "training". Thus each piece will be a cross-valiation set once, but be training set k-1 times. Note that to be more convenient in training, we remove the "id" column since it is an artifical feature we added and is irrelevant to the outcome. 

I used the "Random Forest" algorithm to train the training set, since I found the "rpart" algorithm works poorly in my case. "modFit" is the resulting model and "pred" gives the predicted y. 
```{r}
acyRate = rep(0,k_fold)
for (k in 1:k_fold) {
      training <- subset(datavar, datavar$id != k)
      crossval <- subset(datavar, datavar$id == k)
           
      training <- training[,!(names(training) %in% "id")]
      crossval <- crossval[,!(names(crossval) %in% "id")]
      modFit <- randomForest(classe ~ ., data=training, ntree=100)
      pred <- predict(modFit, crossval)     
      
      checkVector <- pred == crossval$classe
      predRate <- 1.0 - length(checkVector[checkVector==FALSE])/length(checkVector)
      acyRate[k] <- predRate
}
acyRate
mean(acyRate); sd(acyRate)
```
The array "checkVector" is used to compare the predicted outcome $\hat{y}$ and the variable "claase" $y$.
If the prediction is correct, $\hat{y}[i]=y[i]$, then the corresponding element of check array (checkVector$[i]$) will show "TRUE"; otherwise "FALSE". Then we count the number of "TRUE" to qualtitively examine the accruacy of our model.

The number "predRate" gives the ratio, how much percentage the model predicted correctly.
Higher predRates mean better predictions. In our concrete example, k_fold=5, I found the average ratio is about 0.995.. with small errors (< $1 \%$). This explicitly means that, in average, every 100 samples, we may only make one wrong prediction or even less.

The model summary is
```{r}
modFit
```
Here I selected number of tree "ntree=100". I also tried "ntree=200" and the outcome shows similar accuracy. So "ntree=100" is sufficient to give a good prediction. This shows the fitting model is accurate.

In the last, for each different selection of cross-validation, the accuracy ratios are separately
```{r}
acyRate
```
This shows the simulation is stable. Therefore, when I test the test set, I will just choose one of the fitted model, no need to find the model paramters in average.

